import torch
import numpy as np
import torch.nn.functional as F

#rom scripts.dp_loss.diffable_dp_loss 
from diffable_dp_loss import construct_theta, DynamicLossSingle
from sparse_dp_loss import construct_theta_sparse, SparseDynamicLossSingle
from speedy_sparse_dp_loss import construct_theta_sparse_k_loop, SpeedySparseDynamicLossSingle

'''
The DP loss code has undergone several iterative improvements to deal with various problems it has had.
1. Original DP loss function: NOT a smooth approximation
2. Differentiable DP loss: using implementation detailed in DP paper
3. Sparse differentiable DP loss: to deal with large memory requirements
4. Speedy sparse differentiable DP loss: several improvements made to decrease runtime

We will compare these four methods to see how their losses, gradients (when applicable), and runtimes compare
'''

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
mid1 = torch.tensor([
        [1,1],#,0,3],
        [1,0]#,1,0],
        ], dtype=torch.float32)  

mid2 = torch.tensor([
        [1,0],#,1,8],
        [0,1],#0,0],
        ], dtype=torch.float32) 

midis1= []
midis2= []

midis1.append(mid1)
midis2.append(mid2)
midis1.append(mid2)
midis2.append(mid1)

# Three versions of the loss function
dynamic_loss = DynamicLossSingle.apply
sparse_dynamic_loss = SparseDynamicLossSingle.apply
speedy_sparse_dynamic_loss = SpeedySparseDynamicLossSingle.apply


def compare_thetas():
    # compare the thetas generated by the following algorithms:
    #   construct_theta_sparse_k_loop
    #   construct_theta_sparse
    #   construct_theta

    for m1, m2 in zip(midis1, midis2): 
        orig_theta, orig_grad = construct_theta(m1, m2, zero= 0.001)
        sparse_theta, sparse_grad = construct_theta_sparse(m1,m2, device)
        speedy_sparse_theta, speedy_sparse_grad = construct_theta_sparse_k_loop(m1,m2, device) # NOT WORKING!
        sparse_theta = sparse_theta.to_dense()
        speedy_sparse_theta = speedy_sparse_theta.to_dense()
        sparse_theta[sparse_theta==0] = np.NINF
        speedy_sparse_theta[speedy_sparse_theta==0] = np.NINF
        print(sparse_theta, speedy_sparse_theta)
        assert torch.equal(orig_theta, sparse_theta)
        print(torch.eq(sparse_theta, speedy_sparse_theta).sum())
        assert torch.equal(sparse_theta,speedy_sparse_theta)

def compare_losses(mid1, mid2): 
    l2_loss = F.mse_loss(mid1, mid2)
    dyn_loss = dynamic_loss(mid1, mid2) #recon, orig
    l2_loss.backward()
    print("L2:", l2_loss.data, l2_loss.grad)
    print("Dynamic:", dyn_loss.data)

if __name__ == "__main__":
    compare_thetas()
